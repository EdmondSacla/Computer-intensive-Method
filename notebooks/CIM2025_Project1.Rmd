---
title: |
  **2nd-Year Master of Statistics and Data Science**
  **Computer Intensive Methods: Final projects (2024/2025)**
subtitle: "**Project 1**"
author: "Mikita Bisliuk (2364811), Edmond Sacla Aide (2159278)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
header-includes:
 - \usepackage{float}
 - \usepackage{titling}
 - \pretitle{\begin{center}\LARGE\includegraphics[width=6cm]{UHasselt.png}\\[\bigskipamount]}
 - \posttitle{\end{center}}
---
\pagebreak

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)

# loading packages
# library(tidyverse)
# library(ggplot2)
library(DAAG)
library(robust)
# library(parallel)
library(glm2)

library(foreach)
library(doParallel)

n_cores <- detectCores()
cluster <- makeCluster(n_cores - 1L)
registerDoParallel(cluster)

GLOBAL_B <- 1000L
```

# Project 1

In this project, in all questions, we focused on the nassCDS data which is a US data from police-reported car crashes (1997-2002) in which there is a harmful event (people or property). Data are restricted to front-seat occupants, include only a subset of the variables recorded. More information about the dataset can be found using the following link: https://www.rdocumentation.org/packages/DAAG/versions/1.22/topics/nassCDS. The data is a part of the DAAG R package. To get an access to the data you first need to install the package. The list of variables names is shown below.

```{r ,echo=TRUE}
data(nassCDS)
nassCDS <- na.omit(nassCDS)
names(nassCDS)
nassCDS$Dead<- ifelse(nassCDS$dead=="dead", 1,0)
```

```{r, echo=TRUE}
dim(nassCDS)
```

## Part 1

Let $Y_i$ be an indicator variable which takes the value of 1 if an occupant died in an accident (the variable *dead*) and zero otherwise and $X_i$ be the age of occupant in years (the variable *ageOFocc*). We consider the following GLM:

$$
g(P(Y_i = 1)) = \beta_0 + \beta_1 X_i
$$

### Question 1.1

> Estimate the model using a classical GLM approach.

As we are dealing with binary outcome, the model is estimated by a GLM with a binomial family. 

```{r, echo=TRUE}
glm.daag <- glm(dead ~ ageOFocc, data = nassCDS, family = "binomial")
summary(glm.daag)
```

The coefficient estimate for `ageOFocc` is 0.021183. This means that for each additional year of age, the log-odds of the outcome (`dead`) increase by 0.021183 or alternatively the odds to die in a car accident increase by approximately 2.14%. The p-value for `ageOFocc` is less than 2e-16, indicating that the effect of age on the outcome is significant at 5% level of significance.

### Question 1.2

> Let $X_{50}$ be the age of occupant for which the probability to die is 0.5, i.e., $P(Y_i = 1) = 0.5$. Estimate $X_{50}$. Use non parametric bootstrap to estimate the distribution of $X_{50}$ and to construct a 95% C.I. for the $X_{50}$

_Estimation of $X_{50}$_:

$$
logit(\pi_i) = log(\frac{\pi_i}{1-\pi_i}) = \beta_0 + \beta_1 x_i \\
$$

$$
\pi_i = 0.5, 0 = \beta_0 + \beta_1 X_{50} \\
$$

$$
X_{50} = -\frac{\beta_0}{\beta_1}
$$


```{r, echo=FALSE}
beta_0 <- coef(glm.daag)[1]
beta_1 <- coef(glm.daag)[2]
sprintf("Median effective level: %.2f", -beta_0 / beta_1)

```

_Non parametric bootstrap_

The bootstrap algorithm is as follow:

 - For each iteration resample the data with replacement.
 
 - Refit the GLM to each bootstrap sample
 
 - Compute $X_{50} = -\frac{\beta_0}{\beta_1}$
 
Finally, we compute the mean of $X_{50}$ across bootstrap samples and construct a 95% C.I. using percentiles method.


```{r, cache=TRUE}
mel.boot <- foreach(i=1:GLOBAL_B, .combine = rbind, .export = c("nassCDS")) %dopar% {
  set.seed(i)
  n <- nrow(nassCDS)
  idx <- sample(n, replace = TRUE)
  model <- glm(dead ~ ageOFocc, data = nassCDS[idx, ], family = binomial(link = "logit"))
  -coef(model)[1] / coef(model)[2]
}
```

The histogram illustrates the distribution of $X_{50}$, 95% confidence interval, observed and estimated via bootstrap median effective levels. The estimated bootstrap value for $X_{50}$ is 185.74 which is very close to the observed value of 184.49. To find 95% CI we calculate 2.5% and 97.5% quantiles from bootstrap results giving [166.82, 209.25].

```{r}
hist(mel.boot, probability = TRUE, nclass = 50, main = "Histogram of median effective level X50")
 
abline(v = quantile(mel.boot, probs = c(0.025, 0.975)), col = "blue", lty = "dashed")
abline(v = mean(mel.boot), col = "blue")
abline(v = -beta_0 / beta_1, col = "red")
```

### Question 1.3

> For the model formulated above, estimate the OR (for a unit increased in age). Use non parametric bootstrap to construct a 95% C.I. for the OR (for a unit increased in age) using the percentile and bootstrap *t* interval methods, which one do you prefer for the parameter OR ?

_OR for a unit increased in age_

The OR for a unit increased in age is calculated as the exponentiation value of $\beta_1$:

$$
logit(\pi_i) = log(\frac{\pi_i}{1-\pi_i}) = \beta_0 + \beta_1 x_i \\
$$

$$
odds_i = \frac{\pi_i}{1-\pi_i} = exp(\beta_0 + \beta_1 x_i)
$$


```{r}
beta_1 <- coef(glm.daag)[2]
OR <- exp(beta_1)
sprintf("OR for a unit increased in age: %.2f", OR)
```

_Non parametric bootstrap to construct a 95% C.I using percentile method_
 
 - Resample the data with replacement.
 
 - Fit the GLM to each bootstrap sample.
 
 - Extract $\beta_1$ estimate from each bootstrap fit and compute the OR for each sample.
 
 - Use the 2.5th and 97.5th percentiles of the bootstrap distribution of OR to construct the CI.

_Non parametric bootstrap to construct a 95% C.I:  t interval methods_

 - Resample the data with replacement.
 
 - Fit the GLM to each bootstrap sample.
 
 - Extract $\beta_1$ and the related standard error from each bootstrap fit for each sample.
 
 - Evaluate for each bootstrap t statistic as $(\beta_{1.b}-\beta_1)/se(\beta_1)$
 
 - Calculate the 2.5th and 97.5th percentiles of the bootstrap distribution of t statistics.
 
 - Use found percentiles to construct C.I of logOR, take exp() to find C.I. for OR.

```{r, cache=TRUE}
summary_beta1.boot <- foreach(i=1:GLOBAL_B, .combine = rbind, .export = c("nassCDS")) %dopar% {
  set.seed(i)
  n <- nrow(nassCDS)
  idx <- sample(n, replace = TRUE)
  model <- glm(dead ~ ageOFocc, data = nassCDS[idx, ], family = binomial(link = "logit"))
  summary(model)$coef["ageOFocc", ]
}
```

```{r}
beta_1 <- coef(glm.daag)[2]
beta_1_se <- summary(glm.daag)$coef["ageOFocc", "Std. Error"]

t.boot <- (summary_beta1.boot[,"Estimate"] - beta_1) / summary_beta1.boot[,"Std. Error"]
up <- quantile(t.boot, probs = c(0.975))
lo <- quantile(t.boot, probs = c(0.025))

print("95% C.I. using bootstrap t-interval method:")
print(unname(exp(c(beta_1 + beta_1_se * lo, beta_1 + beta_1_se * up))))

print("95% C.I. using bootstrap percentile method:")
print(unname(quantile(exp(summary_beta1.boot[,"Estimate"]), probs = c(0.025, 0.975))))

```

The bootstrap t interval is slightly wider compared to the bootstrap percentile interval. We prefer the bootstrap percentile interval. The percentile method is generally preferred for the odds ratio, as the bootstrap-t method assumes symmetry and might not account for the skewness in the OR distribution. In general, `Studentized intervals` do not respect transformation of the form $\phi = m(\theta)$


### Question 1.4

> We focus on the odds ratio (OR) for a unit increased in age. Use parametric bootstrap to test the null hypothesis $H_0: OR = 1$.

Null hypothesis ($H_0$): : The odds ratio for a unit increase in age (
OR) is 1. This implies that $\beta1=0$ in the logistic regression model.

Alternative Hypothesis ($H_1$):The odds ratio is not equal to 1 ($\beta1 \neq0$).

The bootstrap procedure is as follow: 

 - Fit the GLM to the observed data under $H_0$, where $\beta1=0$
 
 - Simulate response data using the fitted model under $H_0$
 
 - Refit the GLM to each simulated dataset and calculate the test statistic for ($\beta1$)
 
 - Compare the observed test statistic to the distribution of test statistics from the bootstrap samples.

The summary of null model (of no age effect) is given below: 

```{r, echo=FALSE}
glm.daag0 <- glm(dead ~ 1, data = nassCDS, family = binomial(link = "logit"))
summary(glm.daag0)

inv.logit <- function(x) {
  exp(x) / (1 + exp(x))
}

prob_h0 <- inv.logit(coef(glm.daag0)[1])
```

The probability of death under $H_0$ is equal to exp(-3.04867)/(1+exp(-3.04867)) = 0.0453 at each age level (i.e., the overall prevalence in the sample). Use it for resampling step in parametric bootstrap.

```{r, cache=TRUE}
coef_h0.boot <- foreach(i=1:GLOBAL_B, .combine = rbind, .export = c("nassCDS", "prob_h0")) %dopar% {
  set.seed(i)
  dead.b <- rbinom(nrow(nassCDS), 1, prob_h0)
  model <- glm(dead.b ~ nassCDS$ageOFocc, family = binomial(link = "logit"))
  summary(model)$coef[2]
}
```

The distributions of bootstrap replicates for $\hat{\beta_b}$ is shown below. The Monte-Carlo p-value is equal to $\frac{1}{B + 1}$ and, hence, we reject the null hypothesis.

```{r}
hist(coef_h0.boot, nclass = 50,
     xlim = c(-0.03, 0.03), main = "Histogram of bootstrap beta1")
abline(v = beta_1, col = "red")

# Calculate p-value
p_value <- (1 + sum(coef_h0.boot > beta_1) ) / (GLOBAL_B + 1) # <0.001

```

### Question 1.5

> Let $\pi_{33}$ be the probability of death for an occupant at age 33. Use parametric bootstrap to calculate the standard error for $\pi_{33}$ and construct a 90% C.I. for $\pi_{33}$.

The logistic regression model gives:
$$
\pi_{33} = \frac{\exp(\beta_0 + \beta_1 \cdot 33)}{1 + \exp(\beta_0 + \beta_1 \cdot 33)}
$$

The bootstrap procedure is described as follow:

 - Fit the GLM to the observed data and estimate $\beta_0$ and $\beta_1$ 
 
 - Simulate response data based on the fitted probabilities under the original model.
 
 - Refit the GLM to each bootstrap sample.

 - Calculate $\pi_{33}$ for each refitted model.
 
 - Estimate the standard error (SE) of $\pi_{33}$ from the bootstrap distribution.
 
 - Construct the 90% CI using the bootstrap percentiles


```{r, cache=TRUE}
pi33.boot <- foreach(i=1:GLOBAL_B, .combine = c, .export = c("nassCDS", "glm.daag")) %dopar% {
  set.seed(i)
  dead.boot <- rbinom(nrow(nassCDS), 1, predict(glm.daag, type = "response"))
  model <- glm(dead.boot ~ nassCDS$ageOFocc, family = binomial(link = "logit"))
  predict(model, newdata = data.frame(ageOFocc = 33.0), type = "response")
}
```

```{r}
sprintf("Standard error for pi_33 is: %.2f", sqrt(var(pi33.boot)) )
sprintf("90%% CI for pi_33 is: [%.3f, %.3f]", quantile(pi33.boot,probs=c(0.05)), quantile(pi33.boot,probs=c(0.95)))
```

```{r}
hist(pi33.boot, nclass=50)
abline(v = quantile(pi33.boot, probs=c(0.05,0.950)), col = "blue")
```

## Part 2

In this question we fit a robust GLM for the model specified in Q1. Use the R package *glmRob* to fit the model.

### Question 2.1

> Estimate the model using the R package glmRob.

```{r}
rob.glm.daag <- glmRob(dead ~ ageOFocc, data = nassCDS, family = "binomial")
summary(rob.glm.daag)
```

### Question 2.2

> Use non parametric bootstrap to estimate the SE for the intercept and slope.

The bootstrap algorithm is as follow:

 - Resample the data with replacement.
 
 - Refit the robust GLM to each bootstrap sample
 
 - Compute the intercept and the slope for each bootstrap sample
 
 - Generate the empirical distribution of the intercept and the slope
 
 - Compute the mean of intercept and the slope across bootstrap samples


```{r, cache=TRUE}
rob.coef.boot <- foreach(i=1:GLOBAL_B, .combine = rbind, .export = c("nassCDS"), .packages = c("robust")) %dopar% {
  set.seed(i)
  idx <- sample(nrow(nassCDS), replace = TRUE)
  model <- glmRob(dead ~ ageOFocc, data = nassCDS[idx, ], family = binomial(link = "logit"))
  coef(model)
}
```

```{r}
sprintf("SE for intercept: %.5f", sqrt(var(rob.coef.boot[,1])) )
sprintf("SE for slope: %.5f", sqrt(var(rob.coef.boot[,2])) )
```


```{r}
par(mfrow=c(1,2))
hist(rob.coef.boot[,1], main="alpha",nclass=25)
abline(v = quantile(rob.coef.boot[,1], probs = c(0.025, 0.975)), col = "blue")

hist(rob.coef.boot[,2], main="beta",nclass=25)
abline(v = quantile(rob.coef.boot[,2], probs = c(0.025, 0.975)), col = "blue")
```

### Question 2.3

> Use the jackknife and the bootstrap procedures to estimate the bias and MSE for the intercept and slope estimated by: (1) the GLM model in Q2.1 and (2) the robust GLM model estimated in Q2.2. Which method you prefer to use for the estimation of the intercept and slope?

For each GLM approach, the bias and MSE were evaluated after the sampling procedures: bootstrap and Jackknife.

_Jackknife:_

- Remove one observation at a time.

- Refit the model and calculate the estimates.

- Use jackknife estimates to compute bias and MSE. Also account for inflation factor.

_Bootstrap:_

- Resample data (with replacement) many times.

- Refit the model on each bootstrap sample.

- Use bootstrap estimates to compute bias and MSE.

```{r, cache=TRUE}
coef.boot <- foreach(i=1:GLOBAL_B, .combine = rbind, .export = c("nassCDS")) %dopar% {
  set.seed(i)
  n <- nrow(nassCDS)
  idx <- sample(n, replace = TRUE)
  model <- glm(dead ~ ageOFocc, data = nassCDS[idx, ], family = binomial(link = "logit"))
  coef(model)
}

# take rob.coef.boot from Q 2.2
```

```{r, cache=TRUE}
set.seed(2025)
n <- sample(nrow(nassCDS), size = GLOBAL_B, replace = FALSE)

res.jack <- foreach(i=n, .combine = function(...) {
  mapply(rbind, ..., SIMPLIFY = FALSE)}, .export = c("nassCDS"), .packages = c("robust")) %dopar% {
  nassCDS_jack <- nassCDS[-i,]
  model_normal <- glm(dead ~ ageOFocc, data = nassCDS_jack, family = "binomial")
  model_robust <- glmRob(dead ~ ageOFocc, data = nassCDS_jack, family = "binomial")
  
  list(coef(model_normal), coef(model_robust))
}

coef.jack <- res.jack[[1]]
rob.coef.jack <- res.jack[[2]]

```

```{r}
beta_0 <- coef(glm.daag)[1]
beta_1 <- coef(glm.daag)[2]

sprintf("Bootstrap intercept bias: %.2e", mean(coef.boot[,1]) - beta_0 )
sprintf("Bootstrap intercept MSE: %.2e", mean((coef.boot[,1]) - beta_0)^2 )

sprintf("Bootstrap slope bias: %.2e", mean(coef.boot[,2]) - beta_1 )
sprintf("Bootstrap slope MSE: %.2e", mean((coef.boot[,2]) - beta_1)^2 )

sprintf("Jackknife intercept bias: %.2e", (GLOBAL_B - 1) * (mean(coef.jack[,1]) - beta_0 ) )
sprintf("Jackknife intercept MSE: %.2e", (GLOBAL_B - 1) * (mean((coef.jack[,1]) - beta_0 ))^2 )

sprintf("Jackknife slope bias: %.2e", (GLOBAL_B - 1) * (mean(coef.jack[,2]) - beta_1 ) )
sprintf("Jackknife slope bias: %.2e", (GLOBAL_B - 1) * (mean((coef.jack[,2]) - beta_1 ))^2 )
```
```{r}
rob_beta_0 <- coef(rob.glm.daag)[1]
rob_beta_1 <- coef(rob.glm.daag)[2]

sprintf("Bootstrap intercept bias: %.2e", mean(coef.boot[,1]) - rob_beta_0 )
sprintf("Bootstrap intercept MSE: %.2e", mean((coef.boot[,1]) - rob_beta_0)^2 )

sprintf("Bootstrap slope bias: %.2e", mean(coef.boot[,2]) - rob_beta_1 )
sprintf("Bootstrap slope MSE: %.2e", mean((coef.boot[,2]) - rob_beta_1)^2 )

sprintf("Jackknife intercept bias: %.2e", (GLOBAL_B - 1) * (mean(coef.jack[,1]) - rob_beta_0 ) )
sprintf("Jackknife intercept MSE: %.2e", (GLOBAL_B - 1) * (mean((coef.jack[,1]) - rob_beta_0 ))^2 )

sprintf("Jackknife slope bias: %.2e", (GLOBAL_B - 1) * (mean(coef.jack[,2]) - rob_beta_1 ) )
sprintf("Jackknife slope bias: %.2e", (GLOBAL_B - 1) * (mean((coef.jack[,2]) - rob_beta_1 ))^2 )
```

## Part 3

In this question we focus of the following 2 × 2 table (for the complete case analysis) for the variables airbag and dead.


### Question 3.1

Define the observation unit ($X_i$, $Y_i$) for the question

```{r}
bag.dead <- matrix(c(11058, 13825, 669, 511), nrow = 2, 
                dimnames = list(c("none", "airbag"), c("alive", "dead")))

addmargins(bag.dead)
```

### Question 3.2

> Calculate the odds ratio for usage of airbag and the accident outcome (dead/alive) and construct 95% confidence interval. You can use the R function oddsratio. What is your conclusions? Do you think that airbags in the car influence the accident outcome ?

```{r}
epitools::oddsratio(bag.dead, method = "wald")
```

The OR is less than 1, airbags are associated with decreased odds of survival.

### Question 3.3

> Use parametric bootstrap to construct a construct a 95% confidence interval for the OR.

Simulate B samples from the multinomial distribution using the observed cell proportions. 
For each bootstrap sample:

  - Reconstruct the contingency table.
  
  - Compute the odds ratio

```{r, cache=TRUE}
B <- 1000L
n.nobag <- 669 + 11058
n.airbag <- 511 + 13825
p.nobag <- 669 / n.nobag
p.airbag <- 511 / n.airbag
ors.b <- numeric(B)
set.seed(2025)
ors.b <- foreach(1:B, .combine = c, .packages = c('epitools'), .export = c("n.nobag", "p.nobag", "n.airbag", "p.airbag")) %dopar% {
  s1 <- sum(rbinom(n.nobag, 1, p.nobag))
  s2 <- sum(rbinom(n.airbag, 1, p.airbag))
  
  epitools::oddsratio(c(n.nobag - s1, s1, n.airbag - s2, s2))$measure[2,1]
  
}
```

```{r}
# mean(ors.b)
quantile(ors.b, probs = c(0.025, 0.975))
hist(ors.b, nclass = 50)
abline(v = quantile(ors.b, probs = c(0.025, 0.975)), col = "red")
```

### Question 3.4

> Use permutations test to test the hypothesis that airbags in the car DO NOT influence the accident outcome using a chi-square test for a 2 × 2 table. Compare the distribution of the chi-square test statistic in this question to the theoretical distribution of the test statistic.

Null Hypothesis ($H_0$): Airbags have no effect on the accident outcome (i.e., the rows and columns of the table are independent).
The permutation test est performed as follow:

 - Use the observed 2X2 table, table to calculate the test statistic
 
 - Permutations Under the Null:
    
    - Shuffle the outcome labels ("Alive" or "Dead") randomly, keeping the marginal totals fixed.
    - For each permutation, create a new 2X2 table and compute the test statistic
    
 - Calculate the p-value:
 
    - Compare the observed test statistic to the distribution of test statistics from the permutations.


```{r}
# Observed chi-square test statistic
observed_test <- chisq.test(bag.dead, correct = FALSE)
observed_stat <- observed_test$statistic
```


```{r, cache=TRUE}
# Permutation test
set.seed(2025)
B <- 1000
permuted_stats <- numeric(B)

# Marginal totals
row_totals <- rowSums(bag.dead)
col_totals <- colSums(bag.dead)
total <- sum(bag.dead)

for (i in 1:B) {
  # Generate a permuted table under the null hypothesis
  permuted_table <- matrix(rmultinom(1, total, prob = outer(row_totals, col_totals, "*") / total^2),
                           nrow = 2)
  
  # Compute chi-square test statistic for permuted table
  permuted_test <- chisq.test(permuted_table, correct = FALSE)
  permuted_stats[i] <- permuted_test$statistic
}

# Calculate p-value
p_value<- (1+sum(permuted_stats>observed_stat))/(B+1)
sprintf("p_value: %.2e", p_value)
```

We reject the $H_0$ => airbags in the car influences significantly the accident outcome.

The distributions align well, the theoretical chi-square distribution is a good approximation for the test statistic under the null hypothesis. 

```{r}
# Plot the distributions
hist(permuted_stats, breaks = 30, probability = TRUE, 
     col = "skyblue", main = "Comparison of Chi-Square Distributions",
     xlab = "Chi-Square Test Statistic")
curve(dchisq(x, df = 1), col = "red", lwd = 2, add = TRUE)

legend("topright", legend = c("Permutation Test (Empirical)", "Chi-Square (Theoretical)"),
       col = c("skyblue", "red"), lwd = 2)
```


## Part 4

If this question we focus on the variables dead (the outcome of the accident) and the gender (the variable sex).


### Question 4.1

> Estimate the proportion of male (𝜋𝑀) and female (𝜋𝐹) that died in the accidents.

To estimate the proportion of males (𝜋𝑀) and female (𝜋𝐹) that died in the accidents, we calculated the proportion of deaths within each gender category. The formula for each proportion is:

$$
\pi_M = \frac{\text{Number of males who died}}{\text{Total number of males}}
$$

$$
\pi_F = \frac{\text{Number of females who died}}{\text{Total number of females}}
$$

```{r}
sex.died <- with(nassCDS, table(sex, dead))
n.total <- sum(sex.died)
n.male <- 716 + 13253
n.female <- 464 + 11784
p.male <- 716 / n.male
p.female <- 464 / n.female

sprintf("The proportion of male  that died in the accidents is  : %.3f", p.male)

sprintf("The proportion of female that died in the accidents is  : %.3f", p.female)
```

### Question 4.2

> Test the hypothesis that the proportion of male and female that died in an accident are equal using a classical two-samples test (use a two sided test).

Null hypothesis ($H_0$): $𝜋_𝑀=𝜋_F$
Alternative hypothesis ($H_A$): $𝜋_𝑀\neq𝜋_F$

```{r}
# Data
f_alive <- 11784
f_dead <- 464
m_alive <- 13253
m_dead <- 716

# Total counts
f_total <- f_alive + f_dead
m_total <- m_alive + m_dead

# Proportions
pi_F <- f_dead / f_total
pi_M <- m_dead / m_total

# Pooled proportion
pooled_pi <- (f_dead + m_dead) / (f_total + m_total)

# Test statistic
z_stat <- (pi_M - pi_F) / sqrt(pooled_pi * (1 - pooled_pi) * (1 / f_total + 1 / m_total))

# p-value for two-sided test
p_value <- 2 * (1 - pnorm(abs(z_stat)))


# Decision
if (p_value < 0.05) {
  cat("Reject the null hypothesis: The proportions are significantly different.\n")
} else {
  cat("Fail to reject the null hypothesis: No significant difference in proportions.\n")
}
```


### Question 4.3

> Use parametric bootstrap to test the hypothesis that the proportion of male and female that died in an accidents are equal against a two sided alternative.

Under $H_0$, the proportion of males and females who died are the same. The pooled proportion $\pi^{hat}$ is the same for both groups. The bootstrap procedure is as follow:
 - Under the null, generate new datasets by randomly sampling deaths for both males and females based on the pooled proportion $\pi^{hat}$, while keeping the sample sizes fixed (the number of males and females).
 - For each bootstrap sample, calculate the difference in proportions between the males and females who died.
 - Compute the p-value by comparing the observed test statistic to the distribution of the bootstrapped test statistics and make decision.

```{r}
set.seed(2025)
# Bootstrap settings
B <- 1000L
boot_stat <- numeric(B)

# Bootstrap process
for (i in 1:B) {
  # Simulate deaths based on the pooled proportion
  f_boot <- rbinom(1, f_total, pooled_pi)
  m_boot <- rbinom(1, m_total, pooled_pi)
  
  # Compute the difference in proportions for the bootstrap sample
  boot_stat[i] <- (f_boot / f_total) - (m_boot / m_total)
}

# Observed difference in proportions
obs_diff <- pi_F - pi_M

# Compute p-value by comparing observed difference to bootstrap distribution
p_value <-(1+sum(abs(boot_stat) > abs(obs_diff)))/(B+1)

# Conclusion based on p-value
if (p_value < 0.05) {
  cat("Reject the null hypothesis: The proportions are significantly different.\n")
} else {
  cat("Fail to reject the null hypothesis: No significant difference in proportions.\n")
}
```

p-value=9.999e-05. The p-value is less than 0.05, you would reject the null hypothesis and conclude that there is a statistically significant difference in the proportions of males and females who died in accidents.

### Question 4.4

> Use non-parametric bootstrap construct a 95% confident interval for 𝜋𝑀 − 𝜋𝐹

The non-parametric bootstrap procedure to construct a 95% confident interval for $𝜋𝑀 − 𝜋𝐹$ is described as follow:

 - calculate the observed difference in proportions $𝜋𝑀 − 𝜋𝐹$ from the original data. 
 - Create bootstrap samples by resampling with replacement from the observed data for both males and females.
 - For each bootstrap sample, calculate the difference in proportions between males and females.
 - Construct the 95\% percentiles Confidence Interval.

```{r}
set.seed(2025)

# Observed difference in proportions
obs_diff <- pi_M - pi_F

# Bootstrap settings
B <- 1000L
boot_diff <- numeric(B)

# Non-parametric bootstrap process
for (i in 1:B) {
  # Resample males and females with replacement
  f_boot <- sample(c(rep(1, f_dead), rep(0, f_total - f_dead)), f_total, replace = TRUE)
  m_boot <- sample(c(rep(1, m_dead), rep(0, m_total - m_dead)), m_total, replace = TRUE)
  
  # Compute the difference in proportions for the bootstrap sample
  boot_diff[i] <- mean(f_boot) - mean(m_boot)
}

# Calculate the 95% confidence interval from the bootstrap distribution
hist(boot_diff,nclass=100)
CI<- quantile(boot_diff,probs=c(0.025,0.975))
abline(v = CI, col = "blue")
```

The CI 95\% CI: [-0.018, 0.008]. 
The CI does not contain zero: Reject the null hypothesis, suggesting a significant difference between the proportions of males and females who died.

```{r}
stopCluster(cluster)
```

